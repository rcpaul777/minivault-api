{"timestamp": "2025-07-26T06:50:39.708973", "prompt": "What is your name?", "response": "I'm a local AI model, running offline!"}
{"timestamp": "2025-07-26T06:51:42.869508", "prompt": "What is your name?", "response": "I'm a local AI model, running offline!"}
{"timestamp": "2025-07-26T06:53:57.264165", "prompt": "What is your name?", "response": "I'm a local AI model, running offline!"}
{"timestamp": "2025-07-26T06:55:11.641192", "prompt": "Hello, who are you?", "response": "I'm a local AI model, running offline!"}
{"timestamp": "2025-07-26T07:10:10.716995", "prompt": "Hello", "response": "I'm a local AI model, running offline!"}
{"timestamp": "2025-07-26T07:13:02.260997+00:00", "prompt": "Hello", "response": "I'm a local AI model, running offline!"}
{"timestamp": "2025-07-26T07:18:09.501174+00:00", "prompt": "Hello", "response": "I'm a local AI model, running offline!"}
{"timestamp": "2025-07-26T09:43:53.747653+00:00", "prompt": "Hello", "response": "I'm a local AI model, running offline!"}
{"timestamp": "2025-07-26T09:44:16.264918+00:00", "prompt": "Hi", "response": "I'm a local AI model, running offline!"}
{"timestamp": "2025-07-26T18:53:57.329634+00:00", "source": "api", "model": "llama2", "base_url": "http://localhost:8000", "prompt": "Hello, how are you?", "response": " Hello! *smiling* I'm just an AI, I don't have feelings or emotions like humans do, so I can't really feel how I am. However, I'm here and ready to help you with any questions or tasks you may have! How can I assist you today?"}
{"timestamp": "2025-07-26T19:00:51.738648+00:00", "source": "api", "model": "llama2", "base_url": "http://localhost:8000", "prompt": "Hello, who are you?", "response": "\nHello! I'm LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm here to help you with any questions or topics you'd like to discuss. How may I assist you today?"}
{"timestamp": "2025-07-26T19:25:54.838516+00:00", "source": "api", "model": "llama2", "base_url": "http://localhost:8000", "prompt": "ping", "response": "Ping response:\n\nReply from 8.8.8.8: destination host unreachable\n\nPlease let me know if you need any further assistance!"}
{"timestamp": "2025-07-26T19:37:19.885859+00:00", "source": "api", "base_url": "http://localhost:8000", "prompt": "Status", "response": {"app": {"start_time": "2025-07-27T01:07:06.079089", "uptime": "00:00:09", "api_url": "http://localhost:8000"}, "system": {"cpu_percent": 20.0, "per_cpu_percent": [4.6, 10.8, 14.1, 15.6, 10.8, 18.5, 14.1, 13.6], "memory_percent": 84.5, "available_memory_mb": 1232.35, "disk_percent": 65.2, "available_disk_gb": 165.59}, "ollama": {"status": "reachable", "available_models": ["llama2:latest"]}}}
{"timestamp": "2025-07-26T19:37:29.290221+00:00", "source": "api", "base_url": "http://localhost:8000", "prompt": "Status", "response": {"app": {"start_time": "2025-07-27T01:07:06.079089", "uptime": "00:00:19", "api_url": "http://localhost:8000"}, "system": {"cpu_percent": 49.5, "per_cpu_percent": [70.6, 61.2, 78.6, 68.7, 71.6, 64.1, 70.3, 66.2], "memory_percent": 87.2, "available_memory_mb": 1025.37, "disk_percent": 65.2, "available_disk_gb": 165.59}, "ollama": {"status": "reachable", "available_models": ["llama2:latest"]}}}
{"timestamp": "2025-07-26T19:37:53.201215+00:00", "source": "api", "base_url": "http://localhost:8000", "prompt": "Status", "response": {"app": {"start_time": "2025-07-27T01:07:06.079089", "uptime": "00:00:43", "api_url": "http://localhost:8000"}, "system": {"cpu_percent": 24.3, "per_cpu_percent": [10.6, 20.9, 25.8, 18.5, 18.8, 14.1, 27.3, 24.6], "memory_percent": 87.0, "available_memory_mb": 1037.53, "disk_percent": 65.2, "available_disk_gb": 165.59}, "ollama": {"status": "reachable", "available_models": ["llama2:latest"]}}}
{"timestamp": "2025-07-26T19:38:01.688313+00:00", "source": "api", "base_url": "http://localhost:8000", "prompt": "Status", "response": {"app": {"start_time": "2025-07-27T01:07:06.079089", "uptime": "00:00:51", "api_url": "http://localhost:8000"}, "system": {"cpu_percent": 22.5, "per_cpu_percent": [21.2, 13.8, 7.8, 25.8, 26.2, 14.1, 34.4, 25.0], "memory_percent": 87.0, "available_memory_mb": 1038.32, "disk_percent": 65.2, "available_disk_gb": 165.59}, "ollama": {"status": "reachable", "available_models": ["llama2:latest"]}}}
{"timestamp": "2025-07-26T19:39:17.219950+00:00", "source": "cli", "base_url": "http://localhost:8000", "prompt": "Status", "response": {"app": {"start_time": "2025-07-27T01:07:06.079089", "uptime": "00:02:07", "api_url": "http://localhost:8000"}, "system": {"cpu_percent": 16.9, "per_cpu_percent": [9.2, 3.1, 12.5, 20.3, 14.1, 6.2, 28.1, 23.4], "memory_percent": 85.8, "available_memory_mb": 1129.3, "disk_percent": 65.2, "available_disk_gb": 165.58}, "ollama": {"status": "reachable", "available_models": ["llama2:latest"]}}}
{"timestamp": "2025-07-26T19:39:58.286587+00:00", "source": "cli", "base_url": "http://localhost:8000", "prompt": "Status", "response": {"app": {"start_time": "2025-07-27T01:09:45.858998", "uptime": "00:00:08", "api_url": "http://localhost:8000"}, "system": {"cpu_percent": 17.6, "per_cpu_percent": [10.8, 21.9, 7.8, 15.6, 32.3, 3.1, 14.1, 12.7], "memory_percent": 86.2, "available_memory_mb": 1101.56, "disk_percent": 65.2, "available_disk_gb": 165.58}, "ollama": {"status": "reachable", "available_models": ["llama2:latest"]}}}
{"timestamp": "2025-07-26T19:40:09.316264+00:00", "source": "api", "base_url": "http://localhost:8000", "prompt": "Status", "response": {"app": {"start_time": "2025-07-27T01:09:45.858998", "uptime": "00:00:19", "api_url": "http://localhost:8000"}, "system": {"cpu_percent": 27.6, "per_cpu_percent": [33.3, 26.6, 38.5, 37.5, 45.3, 26.6, 40.9, 44.4], "memory_percent": 86.7, "available_memory_mb": 1057.66, "disk_percent": 65.2, "available_disk_gb": 165.58}, "ollama": {"status": "reachable", "available_models": ["llama2:latest"]}}}
{"timestamp": "2025-07-26T19:40:44.404019+00:00", "source": "api", "base_url": "http://localhost:8000", "prompt": "Status", "response": {"app": {"start_time": "2025-07-27T01:09:45.858998", "uptime": "00:00:54", "api_url": "http://localhost:8000"}, "system": {"cpu_percent": 19.3, "per_cpu_percent": [23.2, 16.7, 28.8, 21.9, 18.8, 23.1, 20.0, 24.6], "memory_percent": 86.1, "available_memory_mb": 1112.68, "disk_percent": 65.2, "available_disk_gb": 165.58}, "ollama": {"status": "reachable", "available_models": ["llama2:latest"]}}}
{"timestamp": "2025-07-26T19:56:39.713369+00:00", "source": "test", "model": "llama2", "prompt": "What's your name?", "response": "I'm just an AI, I don't have a personal name. However, you can call me \"Assistant\" or \"AI\" if you like!"}
{"timestamp": "2025-07-26T19:56:54.637269+00:00", "source": "test", "prompt": "Status", "response": {"app": {"start_time": "2025-07-27T01:25:16.264177", "uptime": "00:01:34", "api_url": "http://localhost:8000"}, "system": {"cpu_percent": 52.6, "per_cpu_percent": [71.0, 67.7, 74.6, 70.6, 76.9, 64.6, 78.5, 69.7], "memory_percent": 97.8, "available_memory_mb": 176.3, "disk_percent": 65.2, "available_disk_gb": 165.59}, "ollama": {"status": "reachable", "available_models": ["llama2:latest"]}}}
{"timestamp": "2025-07-26T20:16:55.436696+00:00", "source": "test", "model": "llama2", "prompt": "What's your name?", "response": "I'm just an AI, I don't have a personal name. My purpose is to assist and provide helpful responses to the best of my abilities based on my training. How can I help you today?"}
{"timestamp": "2025-07-26T20:17:04.731202+00:00", "source": "test", "prompt": "Status", "response": {"app": {"start_time": "2025-07-27T01:44:24.402738", "uptime": "00:02:36", "api_url": "http://localhost:8000"}, "system": {"cpu_percent": 43.4, "per_cpu_percent": [47.8, 34.8, 41.5, 34.4, 64.1, 21.9, 67.7, 34.4], "memory_percent": 98.4, "available_memory_mb": 131.25, "disk_percent": 65.2, "available_disk_gb": 165.58}, "ollama": {"status": "reachable", "available_models": ["llama2:latest"]}}}
{"timestamp": "2025-07-26T20:37:41.670607+00:00", "source": "test", "model": "llama2", "prompt": "What's your name?", "response": "I'm just an AI, I don't have a personal name. My purpose is to assist and provide helpful responses to the best of my ability. Is there anything else I can help you with?"}
{"timestamp": "2025-07-26T20:37:46.552049+00:00", "source": "test", "prompt": "Error", "error": "Model: nonexistent-model - Error: Failed to generate response: Ollama server not running at http://localhost:11434, Please start it using: `ollama run nonexistent-model`", "stack_trace": "Traceback (most recent call last):\n  File \"C:\\Users\\Chetan Paul\\minivault-api\\app\\ollama_client.py\", line 12, in generate_response\n    raise ConnectionError(f\"Ollama server not running at {OLLAMA_BASE_URL}, Please start it using: `ollama run {model}`\")\nConnectionError: Ollama server not running at http://localhost:11434, Please start it using: `ollama run nonexistent-model`\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Chetan Paul\\minivault-api\\app\\main.py\", line 57, in generate\n    promt_generate_response = generate_response(request.prompt, request.model)\n  File \"C:\\Users\\Chetan Paul\\minivault-api\\app\\ollama_client.py\", line 16, in generate_response\n    raise RuntimeError(f\"Failed to generate response: {str(e)}\")\nRuntimeError: Failed to generate response: Ollama server not running at http://localhost:11434, Please start it using: `ollama run nonexistent-model`\n"}
{"timestamp": "2025-07-26T20:37:51.178595+00:00", "source": "test", "prompt": "Status", "response": {"app": {"start_time": "2025-07-27T02:06:38.377338", "uptime": "00:01:08", "api_url": "http://localhost:8000"}, "system": {"cpu_percent": 26.0, "per_cpu_percent": [60.0, 64.7, 70.1, 58.2, 55.2, 55.2, 52.3, 55.4], "memory_percent": 96.1, "available_memory_mb": 307.28, "disk_percent": 65.2, "available_disk_gb": 165.57}, "ollama": {"status": "reachable", "available_models": ["llama2:latest"]}}}
{"timestamp": "2025-07-26T20:41:29.356993+00:00", "source": "test", "model": "llama2", "prompt": "What's your name?", "response": "I'm just an AI, I don't have a personal name. I exist to assist and provide information to users like you, so feel free to call me whatever you'd like!"}
{"timestamp": "2025-07-26T20:41:38.543815+00:00", "source": "test", "prompt": "Status", "response": {"app": {"start_time": "2025-07-27T02:10:43.996539", "uptime": "00:00:50", "api_url": "http://localhost:8000"}, "system": {"cpu_percent": 23.4, "per_cpu_percent": [18.8, 18.8, 26.6, 21.5, 12.5, 7.8, 21.9, 9.4], "memory_percent": 96.4, "available_memory_mb": 290.89, "disk_percent": 65.2, "available_disk_gb": 165.57}, "ollama": {"status": "reachable", "available_models": ["llama2:latest"]}}}
{"timestamp": "2025-07-26T20:46:32.809437+00:00", "source": "test", "model": "llama2", "prompt": "What's your name?", "response": "I'm just an AI, I don't have a personal name. My purpose is to assist and provide helpful responses to the best of my ability. How may I help you today?"}
{"timestamp": "2025-07-26T20:46:37.540097+00:00", "source": "test", "prompt": "Error", "error": "Status Code: 500-Model: nonexistent-model - Error: Failed to generate response: Ollama server not running at http://localhost:11434, Please start it using: `ollama run nonexistent-model`", "stack_trace": "Traceback (most recent call last):\n  File \"C:\\Users\\Chetan Paul\\minivault-api\\app\\ollama_client.py\", line 12, in generate_response\n    raise ConnectionError(f\"Ollama server not running at {OLLAMA_BASE_URL}, Please start it using: `ollama run {model}`\")\nConnectionError: Ollama server not running at http://localhost:11434, Please start it using: `ollama run nonexistent-model`\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Chetan Paul\\minivault-api\\app\\main.py\", line 57, in generate\n    promt_generate_response = generate_response(request.prompt, request.model)\n  File \"C:\\Users\\Chetan Paul\\minivault-api\\app\\ollama_client.py\", line 16, in generate_response\n    raise RuntimeError(f\"Failed to generate response: {str(e)}\")\nRuntimeError: Failed to generate response: Ollama server not running at http://localhost:11434, Please start it using: `ollama run nonexistent-model`\n"}
{"timestamp": "2025-07-26T20:46:41.942039+00:00", "source": "test", "prompt": "Status", "response": {"app": {"start_time": "2025-07-27T02:14:10.590220", "uptime": "00:02:27", "api_url": "http://localhost:8000"}, "system": {"cpu_percent": 25.3, "per_cpu_percent": [18.8, 16.9, 52.3, 21.9, 36.4, 28.1, 28.8, 14.1], "memory_percent": 98.8, "available_memory_mb": 96.29, "disk_percent": 65.2, "available_disk_gb": 165.57}, "ollama": {"status": "reachable", "available_models": ["llama2:latest"]}}}
